{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRDCDH Semantic Mapping with BlazingText Word2Vec \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Word2Vec is a popular algorithm used for generating dense vector representations of words in large corpora using unsupervised learning. The resulting vectors have been shown to capture semantic relationships between the corresponding words and are used extensively for many downstream natural language processing (NLP) tasks like sentiment analysis, named entity recognition and machine translation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by setting SageMaker environment:\n",
    "- For the first time to run the notebook, make sure sageMaker environment configuration file, sagemaker_config.py, exists in src/common directory.  If not, create it by copy sagemaker_config_sample.py file under src/common dir and rename the copy to sagemaker_config.py\n",
    "- Open the sagemaker_config.py under src/common dir, review all environment settings and update them if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /opt/homebrew/share/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/gup2/Library/Application Support/sagemaker/config.yaml\n",
      "src dir:/Users/gup2/workspace/vscode/crdcdh/crdcdh-ml-notebooks/src\n",
      "root dir:/Users/gup2/workspace/vscode/crdcdh/crdcdh-ml-notebooks\n",
      "sagemaker.config INFO - Fetched defaults config from location: /Users/gup2/workspace/vscode/crdcdh/crdcdh-ml-notebooks/configs/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from semantic_analysis_class import SemanticAnalysis\n",
    "import common.sagemaker_config as config\n",
    "\n",
    "# instantiate a class object from SemanticAnalysis and set SageMaker environment\n",
    "semantic_analysis = SemanticAnalysis()\n",
    "s3_bucket = semantic_analysis.get_s3_bucket()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and/or Test Model Setup\n",
    "First of all, set the container by calling:\n",
    "    semantic_analysis.set_container(container_name, container_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: latest.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker BlazingText container: 811284229777.dkr.ecr.us-east-1.amazonaws.com/blazingtext:1 (us-east-1)\n"
     ]
    }
   ],
   "source": [
    "CONTAINER_IMAGE_NAME = \"blazingtext\"\n",
    "CONTAINER_IMAGE_VERSION = \"latest\"\n",
    "semantic_analysis.set_container(CONTAINER_IMAGE_NAME, CONTAINER_IMAGE_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, decide train and/or test model workflow by setting switches:\n",
    "\n",
    "By setting TEST_MODEL_ONLY to false, we decide to train model and test model.  Otherwise, only test trained model, that skips training step, a very costly step.\n",
    "    1) TEST_MODEL_ONLY = False  \n",
    "   \n",
    "By setting TRANSFORM_DATA to false, we decide to use existing training data in text8 format.  Otherwise, need call transformData function to convert json files to a text8 file .\n",
    "    2) TRANSFORM_DATA = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model is existed at train_output/blazingtext-2024-06-12-15-16-09-327/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#setting switches to determine workflow\n",
    "TEST_MODEL_ONLY = True \n",
    "TRANSFORM_DATA = False \n",
    "\n",
    "trained_model_s3_path = \"\"\n",
    "\n",
    "if TEST_MODEL_ONLY == True:\n",
    "    #must set trained model s3 path here\n",
    "    trained_model_s3_path = \"train_output/blazingtext-2024-06-12-15-16-09-327/output/model.tar.gz\"\n",
    "    #check if the model exists\n",
    "    if trained_model_s3_path == \"\" or not s3_bucket.file_exists_on_s3(trained_model_s3_path):\n",
    "        #if not, raise an exception and exit.\n",
    "        print(\"Trained model is not existed!\")\n",
    "        semantic_analysis.close()\n",
    "    else:\n",
    "        print(\"Trained model is existed at \" + trained_model_s3_path)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For train model, need to prepare training data, if only have datasets in json files, need call transform function to create a one training file in text8 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from common.utils import get_data_time\n",
    "trining_data_s3_path = \"\" \n",
    "if TEST_MODEL_ONLY == False:\n",
    "    if TRANSFORM_DATA == True:\n",
    "        # if need to transform data, must set the raw data folder that contain json file(s) either in s3 bucket of in local folder\n",
    "        s3_raw_data_prefix = \"\" #set the prefix of raw data folder in s3 bucket if you have\n",
    "        local_raw_data_folder = \"../data/raw/json/\"\n",
    "        if not s3_raw_data_prefix or not s3_bucket.file_exists_on_s3(s3_raw_data_prefix):\n",
    "            #check if json file in local folder\n",
    "            if not os.path.exists(local_raw_data_folder) or not any(fname.endswith('.json') for fname in os.listdir(local_raw_data_folder)):\n",
    "                print(\"No raw data found on s3 bucket or local folder\")\n",
    "                semantic_analysis.close()\n",
    "\n",
    "        else:\n",
    "            # if in s3 bucket, download to local folder\n",
    "            s3_bucket.download_files_in_folder(s3_raw_data_prefix, local_raw_data_folder)\n",
    "        \n",
    "        local_text8_file_path = \"../data/train/test8/train_data.txt\"\n",
    "        trining_data_s3_path = semantic_analysis.transformData(s3_raw_data_prefix, local_text8_file_path)\n",
    "    else:\n",
    "        # if not need to transform data, must set the training data file either in s3 bucket of in local folder\n",
    "        local_text8_file_path = \"../data/train/text8/updated_all_training_set_gdcfixed.txt\"\n",
    "        trining_data_s3_path = \"data/train/blazingtext-2024-06-12-11-15-48-f/train_data\"\n",
    "        \n",
    "        if not trining_data_s3_path or not s3_bucket.file_exists_on_s3(trining_data_s3_path):\n",
    "            #check if training data file in local folder\n",
    "            if not os.path.exists(local_text8_file_path):\n",
    "                print(\"No training data found on s3 bucket or local folder\")\n",
    "                semantic_analysis.close()\n",
    "            else:\n",
    "                # if in local folder, upload to s3\n",
    "                trining_data_s3_path = os.path.join(config.TRAIN_DATA_PREFIX, f\"{CONTAINER_IMAGE_NAME}-{get_data_time}/train_data\")\n",
    "                try:\n",
    "                    s3_bucket.upload_file(trining_data_s3_path, local_text8_file_path)\n",
    "                except Exception as e:\n",
    "                    print(\"Failed to update training data.\")\n",
    "                    semantic_analysis.close()\n",
    "\n",
    "    print(\"Training data path: \" + trining_data_s3_path)\n",
    "    #call prepare_train_data function\n",
    "    semantic_analysis.prepare_train_data(trining_data_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BlazingText model for generating word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL_ONLY == False:\n",
    "    semantic_analysis.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now download the word vectors learned by our model and visualize them using a [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_MODEL_ONLY == False:\n",
    "    downloaded_model_path = \"../output/model.tar.gz\"\n",
    "    semantic_analysis.download_trained_model(downloaded_model_path) # download trained model to local\n",
    "    semantic_analysis.evaluate_learned_model_vacs(downloaded_model_path, \"../output/model\") # evaluate learned vectors in trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get an n-dimensional vector (where n is vector_dim as specified in hyperparameters) for each of the words. If the word is not there in the training dataset, the model will return a vector of zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above might generate a plot like the one below. t-SNE and Word2Vec are stochastic, so although when you run the code the plot wonâ€™t look exactly like this, you can still see clusters of similar words such as below where 'british', 'american', 'french', 'english' are near the bottom-left, and 'military', 'army' and 'forces' are all together near the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tsne plot of embeddings](../images/tsne.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Inference\n",
    "Once the training is done, we can deploy the trained model as an Amazon SageMaker real-time hosted endpoint. This will allow us to make predictions (or inference) from the model. Note that we don't have to host on the same type of instance that we used to train. Because instance endpoints will be up and running for long, it's advisable to choose a cheaper instance for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.model.Model object at 0x31bcd3cd0>\n",
      "------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = config.ENDPOINT_NAME #default endpoint name, crdcdh-ml-dev-endpoint in dev. The name can be customized by valid unique name.\n",
    "semantic_analysis.deploy_trained_model(endpoint_name, trained_model_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting vector representations for words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use JSON format for inference\n",
    "The payload should contain a list of words with the key as \"**instances**\". BlazingText supports content-type `application/json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[{\"vector\": [-0.08171992003917694, 0.4133380055427551, 0.15874870121479034, 0.9120864272117615, 0.35315394401550293, 0.5989611148834229, -0.13320013880729675, 0.32997578382492065, -0.931456983089447, -0.3034103810787201, -0.6623309254646301, 0.7876906394958496, -0.1892700493335724, 0.354099839925766, -0.4875767230987549, 0.5718265175819397, -0.015936193987727165, -0.2858751714229584, 0.1493469625711441, 0.5570916533470154, 0.05738626420497894, 0.2775570750236511, -0.2805495858192444, 0.8012495636940002, -0.49053627252578735, 0.15655098855495453, 0.6446893811225891, -0.23246367275714874, 0.3104402422904968, 0.336190402507782, -0.2623969614505768, 0.5630905628204346, -0.6810024380683899, -0.3621712327003479, 0.003490819362923503, -0.08670128136873245, 0.08936437964439392, -0.20517677068710327, -0.36212030053138733, 0.5797169804573059, -0.28241923451423645, -0.3633529543876648, 0.4453902542591095, 0.4731803238391876, -0.44600656628608704, -0.5014044046401978, -0.7921153903007507, -0.5792698860168457, 0.03827626630663872, 0.6126625537872314, -0.11414636671543121, -0.1956414431333542, 0.3516545593738556, 0.1700873076915741, 0.38657674193382263, -0.19465939700603485, -0.30789801478385925, -0.8006144165992737, -0.8244677186012268, 0.6613159775733948, 0.057018719613552094, -0.3190752863883972, 0.15397483110427856, 0.4680369794368744, -0.4867040812969208, 0.7190322875976562, -0.38078391551971436, 0.4378146231174469, 0.7814283967018127, -0.05103764683008194, 0.47123560309410095, 0.15454955399036407, -0.052298445254564285, -0.30221572518348694, -0.3798041343688965, -0.01255019661039114, -0.21892204880714417, 0.5025933384895325, 0.2832168936729431, 0.42159417271614075, 0.19957493245601654, 0.1713893562555313, -0.37553438544273376, 0.2501133382320404, -0.22320421040058136, -0.07581277936697006, 0.43988916277885437, -0.13493786752223969, -0.006226875353604555, 0.2820025384426117, 0.3885079026222229, -0.3939996361732483, -0.035369861871004105, 0.2293861359357834, -0.36726081371307373, -0.22884464263916016, 0.28811463713645935, -0.10415034741163254, 0.09286428242921829, -0.24653519690036774], \"word\": \"protocol identifier\"}]'\n",
      "[{'vector': [-0.08171992003917694, 0.4133380055427551, 0.15874870121479034, 0.9120864272117615, 0.35315394401550293, 0.5989611148834229, -0.13320013880729675, 0.32997578382492065, -0.931456983089447, -0.3034103810787201, -0.6623309254646301, 0.7876906394958496, -0.1892700493335724, 0.354099839925766, -0.4875767230987549, 0.5718265175819397, -0.015936193987727165, -0.2858751714229584, 0.1493469625711441, 0.5570916533470154, 0.05738626420497894, 0.2775570750236511, -0.2805495858192444, 0.8012495636940002, -0.49053627252578735, 0.15655098855495453, 0.6446893811225891, -0.23246367275714874, 0.3104402422904968, 0.336190402507782, -0.2623969614505768, 0.5630905628204346, -0.6810024380683899, -0.3621712327003479, 0.003490819362923503, -0.08670128136873245, 0.08936437964439392, -0.20517677068710327, -0.36212030053138733, 0.5797169804573059, -0.28241923451423645, -0.3633529543876648, 0.4453902542591095, 0.4731803238391876, -0.44600656628608704, -0.5014044046401978, -0.7921153903007507, -0.5792698860168457, 0.03827626630663872, 0.6126625537872314, -0.11414636671543121, -0.1956414431333542, 0.3516545593738556, 0.1700873076915741, 0.38657674193382263, -0.19465939700603485, -0.30789801478385925, -0.8006144165992737, -0.8244677186012268, 0.6613159775733948, 0.057018719613552094, -0.3190752863883972, 0.15397483110427856, 0.4680369794368744, -0.4867040812969208, 0.7190322875976562, -0.38078391551971436, 0.4378146231174469, 0.7814283967018127, -0.05103764683008194, 0.47123560309410095, 0.15454955399036407, -0.052298445254564285, -0.30221572518348694, -0.3798041343688965, -0.01255019661039114, -0.21892204880714417, 0.5025933384895325, 0.2832168936729431, 0.42159417271614075, 0.19957493245601654, 0.1713893562555313, -0.37553438544273376, 0.2501133382320404, -0.22320421040058136, -0.07581277936697006, 0.43988916277885437, -0.13493786752223969, -0.006226875353604555, 0.2820025384426117, 0.3885079026222229, -0.3939996361732483, -0.035369861871004105, 0.2293861359357834, -0.36726081371307373, -0.22884464263916016, 0.28811463713645935, -0.10415034741163254, 0.09286428242921829, -0.24653519690036774], 'word': 'protocol identifier'}]\n"
     ]
    }
   ],
   "source": [
    "words = [\"protocol identifier\"]\n",
    "semantic_analysis.test_trained_model(words, endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop / Close the Endpoint (Optional)\n",
    "Finally, for training and test purpose, we need delete the endpoint before we close the notebook.  If you want to host the model in the endpoint for a while, just remove the endpoint_name parameter from the close() as listed below:\n",
    "\n",
    "semantic_analysis.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete the SageMaker endpoint and resources\n",
    "semantic_analysis.close(endpoint_name)\n",
    "semantic_analysis = None\n",
    "s3_bucket = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
